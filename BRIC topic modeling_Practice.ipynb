{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36064056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch torchvision\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce3cc48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6140, 0.8464, 0.4523],\n",
      "        [0.0775, 0.0491, 0.9084],\n",
      "        [0.4156, 0.7022, 0.8700],\n",
      "        [0.3527, 0.0873, 0.0389],\n",
      "        [0.2111, 0.6637, 0.6840]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdddf5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This movie is fantastic! I absolutely loved it. What the fuck?\n",
      "Predicted Probability (Negative): 0.4417\n",
      "Predicted Probability (Positive): 0.5583\n",
      "Predicted Class: Positive\n",
      "Review: The acting was terrible, and the plot was confusing.\n",
      "Predicted Probability (Negative): 0.4693\n",
      "Predicted Probability (Positive): 0.5307\n",
      "Predicted Class: Positive\n",
      "Review: An excellent movie with brilliant performances.\n",
      "Predicted Probability (Negative): 0.4429\n",
      "Predicted Probability (Positive): 0.5571\n",
      "Predicted Class: Positive\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Example data - movie reviews\n",
    "reviews = [\n",
    "    \"This movie is fantastic! I absolutely loved it. What the fuck?\",\n",
    "    \"The acting was terrible, and the plot was confusing.\",\n",
    "    \"An excellent movie with brilliant performances.\"\n",
    "]\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize input reviews and convert to tensors\n",
    "inputs = tokenizer(reviews, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Perform text classification\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get predicted class probabilities\n",
    "predicted_probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "\n",
    "# Display predictions\n",
    "for i, review in enumerate(reviews):\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted Probability (Negative): {predicted_probabilities[i][0]:.4f}\")\n",
    "    print(f\"Predicted Probability (Positive): {predicted_probabilities[i][1]:.4f}\")\n",
    "    predicted_class = torch.argmax(predicted_probabilities[i]).item()\n",
    "    print(f\"Predicted Class: {'Negative' if predicted_class == 0 else 'Positive'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "225cc60d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This movie is fantastic! I absolutely loved it.\n",
      "Predicted Probability (Negative): 0.4556\n",
      "Predicted Probability (Positive): 0.5444\n",
      "Predicted Class: Positive\n",
      "\n",
      "Text: The acting was terrible, and the plot was confusing.\n",
      "Predicted Probability (Negative): 0.5559\n",
      "Predicted Probability (Positive): 0.4441\n",
      "Predicted Class: Negative\n",
      "\n",
      "Text: An excellent movie with brilliant performances.\n",
      "Predicted Probability (Negative): 0.4526\n",
      "Predicted Probability (Positive): 0.5474\n",
      "Predicted Class: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "# Example data - labeled text for sentiment analysis\n",
    "train_texts = [\n",
    "    \"This movie is fantastic! I absolutely loved it.\",\n",
    "    \"The acting was terrible, and the plot was confusing.\",\n",
    "    \"An excellent movie with brilliant performances.\"\n",
    "]\n",
    "train_labels = [1, 0, 1]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # binary classification\n",
    "\n",
    "# Tokenize input texts and convert to tensors\n",
    "inputs = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "labels = torch.tensor(train_labels)\n",
    "\n",
    "# Fine-tune the BERT model\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)  # Choose an appropriate learning rate\n",
    "model.train()\n",
    "for epoch in range(3):  # Adjust the number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Perform inference after fine-tuning\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    predicted_classes = torch.argmax(predicted_probabilities, dim=1)\n",
    "\n",
    "# Display predictions\n",
    "for i, text in enumerate(train_texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Probability (Negative): {predicted_probabilities[i][0]:.4f}\")\n",
    "    print(f\"Predicted Probability (Positive): {predicted_probabilities[i][1]:.4f}\")\n",
    "    print(f\"Predicted Class: {'Negative' if predicted_classes[i] == 0 else 'Positive'}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adeadb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This movie is fantastic! I absolutely loved it.\n",
      "Predicted Probability (Negative): 0.5088\n",
      "Predicted Probability (Positive): 0.4912\n",
      "Predicted Class: Negative\n",
      "\n",
      "Text: The acting was terrible, and the plot was confusing.\n",
      "Predicted Probability (Negative): 0.5473\n",
      "Predicted Probability (Positive): 0.4527\n",
      "Predicted Class: Negative\n",
      "\n",
      "Text: An excellent movie with brilliant performances.\n",
      "Predicted Probability (Negative): 0.4724\n",
      "Predicted Probability (Positive): 0.5276\n",
      "Predicted Class: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "# def set_seed(seed_value=42):\n",
    "#     \"\"\"Set seed for reproducibility.\"\"\"\n",
    "#     random.seed(seed_value)\n",
    "#     np.random.seed(seed_value)\n",
    "#     torch.manual_seed(seed_value)\n",
    "#     torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "# # Set seed\n",
    "# set_seed(seed_value=42)\n",
    "\n",
    "# Rest of your code for BERT fine-tuning and inference goes here...\n",
    "\n",
    "\n",
    "# Example data - labeled text for sentiment analysis\n",
    "train_texts = [\n",
    "    \"This movie is fantastic! I absolutely loved it.\",\n",
    "    \"The acting was terrible, and the plot was confusing.\",\n",
    "    \"An excellent movie with brilliant performances.\"\n",
    "]\n",
    "train_labels = [1, 0, 1]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # binary classification\n",
    "\n",
    "# Tokenize input texts and convert to tensors\n",
    "inputs = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "labels = torch.tensor(train_labels)\n",
    "\n",
    "# Fine-tune the BERT model\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)  # Choose an appropriate learning rate\n",
    "model.train()\n",
    "for epoch in range(3):  # Adjust the number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Perform inference after fine-tuning\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    predicted_classes = torch.argmax(predicted_probabilities, dim=1)\n",
    "\n",
    "# Display predictions\n",
    "for i, text in enumerate(train_texts):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Probability (Negative): {predicted_probabilities[i][0]:.4f}\")\n",
    "    print(f\"Predicted Probability (Positive): {predicted_probabilities[i][1]:.4f}\")\n",
    "    print(f\"Predicted Class: {'Negative' if predicted_classes[i] == 0 else 'Positive'}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6ef51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the PDF file and extract text\n",
    "import os\n",
    "directory_path = 'E:/plant diversity/Forest_management/37' #1_Canopy space filling and tree crown morphology in mixed-species stands compared with monocultures.pdf'\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.pdf')]\n",
    "# pdf_file='E:/plant diversity/Forest_management/37/#1_Canopy space filling and tree crown morphology in mixed-species stands compared with monocultures.pdf'\n",
    "\n",
    "pdf_text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf690317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pdf_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(pdf_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     12\u001b[0m     reader \u001b[38;5;241m=\u001b[39m PyPDF2\u001b[38;5;241m.\u001b[39mPdfReader(file)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mpages:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pdf_file' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import PyPDF2\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "with open(pdf_file, 'rb') as file:\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    for page in reader.pages:\n",
    "        pdf_text += page.extract_text()\n",
    "\n",
    "# Tokenize the text using BERT tokenizer\n",
    "inputs = tokenizer(pdf_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Perform inference using the BERT model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    predicted_classes = torch.argmax(predicted_probabilities, dim=1)\n",
    "\n",
    "# Display predictions\n",
    "print(f\"Text: {pdf_text}\")\n",
    "print(f\"Predicted Probability (Negative): {predicted_probabilities[0][0]:.4f}\")\n",
    "print(f\"Predicted Probability (Positive): {predicted_probabilities[0][1]:.4f}\")\n",
    "print(f\"Predicted Class: {'Negative' if predicted_classes[0] == 0 else 'Positive'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62c3e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probability (Negative): 0.6475\n",
      "Predicted Probability (Positive): 0.3525\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5690\n",
      "Predicted Probability (Positive): 0.4310\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5882\n",
      "Predicted Probability (Positive): 0.4118\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6537\n",
      "Predicted Probability (Positive): 0.3463\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5373\n",
      "Predicted Probability (Positive): 0.4627\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6802\n",
      "Predicted Probability (Positive): 0.3198\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6011\n",
      "Predicted Probability (Positive): 0.3989\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5835\n",
      "Predicted Probability (Positive): 0.4165\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6018\n",
      "Predicted Probability (Positive): 0.3982\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.4655\n",
      "Predicted Probability (Positive): 0.5345\n",
      "Predicted Class: Positive\n",
      "\n",
      "Predicted Probability (Negative): 0.5984\n",
      "Predicted Probability (Positive): 0.4016\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6620\n",
      "Predicted Probability (Positive): 0.3380\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6303\n",
      "Predicted Probability (Positive): 0.3697\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5949\n",
      "Predicted Probability (Positive): 0.4051\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6372\n",
      "Predicted Probability (Positive): 0.3628\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6531\n",
      "Predicted Probability (Positive): 0.3469\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6370\n",
      "Predicted Probability (Positive): 0.3630\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6811\n",
      "Predicted Probability (Positive): 0.3189\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6789\n",
      "Predicted Probability (Positive): 0.3211\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6752\n",
      "Predicted Probability (Positive): 0.3248\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5816\n",
      "Predicted Probability (Positive): 0.4184\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5938\n",
      "Predicted Probability (Positive): 0.4062\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6810\n",
      "Predicted Probability (Positive): 0.3190\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6420\n",
      "Predicted Probability (Positive): 0.3580\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5945\n",
      "Predicted Probability (Positive): 0.4055\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6127\n",
      "Predicted Probability (Positive): 0.3873\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6487\n",
      "Predicted Probability (Positive): 0.3513\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6362\n",
      "Predicted Probability (Positive): 0.3638\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5865\n",
      "Predicted Probability (Positive): 0.4135\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6502\n",
      "Predicted Probability (Positive): 0.3498\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.7039\n",
      "Predicted Probability (Positive): 0.2961\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5684\n",
      "Predicted Probability (Positive): 0.4316\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6650\n",
      "Predicted Probability (Positive): 0.3350\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6387\n",
      "Predicted Probability (Positive): 0.3613\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.5817\n",
      "Predicted Probability (Positive): 0.4183\n",
      "Predicted Class: Negative\n",
      "\n",
      "Predicted Probability (Negative): 0.6735\n",
      "Predicted Probability (Positive): 0.3265\n",
      "Predicted Class: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import PyPDF2\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# List of PDF file paths\n",
    "# pdf_files = ['path_to_pdf_1.pdf', 'path_to_pdf_2.pdf', 'path_to_pdf_3.pdf']  # Add more paths as needed\n",
    "\n",
    "# Iterate through each PDF file\n",
    "status=[]\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_text = ''\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            pdf_text += page.extract_text()\n",
    "\n",
    "    # Tokenize the text using BERT tokenizer\n",
    "    inputs = tokenizer(pdf_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Perform inference using the BERT model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "        predicted_classes = torch.argmax(predicted_probabilities, dim=1)\n",
    "        status.append('Negative' if predicted_classes[0] == 0 else 'Positive')\n",
    "\n",
    "#     # Display predictions for each document\n",
    "#     print(f\"File: {pdf_file}\")\n",
    "#     print(f\"Text: {pdf_text}\")\n",
    "    print(f\"Predicted Probability (Negative): {predicted_probabilities[0][0]:.4f}\")\n",
    "    print(f\"Predicted Probability (Positive): {predicted_probabilities[0][1]:.4f}\")\n",
    "    print(f\"Predicted Class: {'Negative' if predicted_classes[0] == 0 else 'Positive'}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8bd2d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df=pd.DataFrame(status)\n",
    "df_stack=df.stack(level=0) \n",
    "counter=df_stack.value_counts() # set top 10: df_stack.value_counts()[0:10]\n",
    "# plt.bar(counter.index,counter.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d653f62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Positive',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative',\n",
       " 'Negative']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
